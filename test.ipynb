{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at prajjwal1/bert-mini and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.024550175294280052, 'start': 15, 'end': 23, 'answer': 'context.'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline('question-answering', model=\"prajjwal1/bert-mini\", tokenizer=\"prajjwal1/bert-mini\")\n",
    "answer = nlp({'context': 'This is a test context.', 'question': 'What is this?'})\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created and populated with dummy data.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create or open a database\n",
    "conn = sqlite3.connect('users.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL script to create tables and insert data\n",
    "sql_script = \"\"\"\n",
    "-- SQL commands go here\n",
    "CREATE TABLE users (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    username TEXT NOT NULL UNIQUE,\n",
    "    password_hash TEXT NOT NULL,\n",
    "    role TEXT NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE doctors (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    name TEXT NOT NULL,\n",
    "    specialization TEXT NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE appointments (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    user_id INTEGER NOT NULL,\n",
    "    doctor_id INTEGER NOT NULL,\n",
    "    appointment_time TEXT NOT NULL,\n",
    "    FOREIGN KEY (user_id) REFERENCES users (id),\n",
    "    FOREIGN KEY (doctor_id) REFERENCES doctors (id)\n",
    ");\n",
    "\n",
    "INSERT INTO users (username, password_hash, role) VALUES ('john_doe', 'hashed_password', 'patient');\n",
    "INSERT INTO users (username, password_hash, role) VALUES ('jane_smith', 'hashed_password', 'doctor');\n",
    "\n",
    "INSERT INTO doctors (name, specialization) VALUES ('Dr. Alice', 'Cardiology');\n",
    "INSERT INTO doctors (name, specialization) VALUES ('Dr. Bob', 'Neurology');\n",
    "\n",
    "INSERT INTO appointments (user_id, doctor_id, appointment_time) VALUES (1, 1, '2023-09-01 10:00:00');\n",
    "INSERT INTO appointments (user_id, doctor_id, appointment_time) VALUES (1, 2, '2023-09-02 11:00:00');\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL script\n",
    "cursor.executescript(sql_script)\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "print(\"Database created and populated with dummy data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_user_details(username, email, phone, blood_group, age):\n",
    "    conn = sqlite3.connect('users.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"UPDATE users SET email = ?, phone = ?, blood_group = ?, age = ? WHERE username = ?\", (email, phone, blood_group, age, username))\n",
    "    conn.commit()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created and populated with dummy data.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create or open a database\n",
    "conn = sqlite3.connect('users.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL script to create tables\n",
    "sql_script = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS users\n",
    "               (id INTEGER PRIMARY KEY, username TEXT, age INTEGER, email TEXT, blood_group TEXT);\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL script\n",
    "cursor.executescript(sql_script)\n",
    "\n",
    "# Check if the table is empty before inserting\n",
    "cursor.execute(\"SELECT * FROM users\")\n",
    "if cursor.fetchone() is None:\n",
    "    cursor.execute(\"INSERT INTO users (username, age, email, blood_group) VALUES ('Alice', 30, 'alice@example.com', 'A+')\")\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "print(\"Database created and populated with dummy data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('medical_app.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch data from all tables\n",
    "tables = ['users', 'doctors', 'appointments']\n",
    "data = {}\n",
    "\n",
    "# for table in tables:\n",
    "#     cursor.execute(f\"SELECT * FROM {table}\")\n",
    "#     data[table] = cursor.fetchall()\n",
    "cursor.execute(\"SELECT id FROM doctors WHERE name = ?\", ('prakash',))\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Displaying the fetched data\n",
    "print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: duplicate column name: email\n",
      "Error occurred: duplicate column name: blood_group\n",
      "Error occurred: duplicate column name: age\n",
      "Database updated successfully.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Database file path\n",
    "db_path = 'users.db'\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL commands to add new columns\n",
    "alter_commands = [\n",
    "    \"ALTER TABLE users ADD COLUMN email TEXT\",\n",
    "     \"ALTER TABLE users ADD COLUMN phone TEXT\",\n",
    "    \"ALTER TABLE users ADD COLUMN blood_group TEXT\",\n",
    "    \"ALTER TABLE users ADD COLUMN age INTEGER\"\n",
    "]\n",
    "\n",
    "# Execute each command\n",
    "for command in alter_commands:\n",
    "    try:\n",
    "        cursor.execute(command)\n",
    "    except sqlite3.OperationalError as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"Database updated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = 'users.db'\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL command to create tables\n",
    "create_tables_commands = [\n",
    "    '''CREATE TABLE IF NOT EXISTS users (\n",
    "           id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "           username TEXT NOT NULL UNIQUE,\n",
    "           password_hash TEXT NOT NULL,\n",
    "           role TEXT NOT NULL,\n",
    "           email TEXT,\n",
    "           phone TEXT,\n",
    "           blood_group TEXT,\n",
    "           age INTEGER\n",
    "       );''',\n",
    "    '''CREATE TABLE IF NOT EXISTS doctors (\n",
    "           id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "           name TEXT NOT NULL,\n",
    "           specialization TEXT NOT NULL\n",
    "       );''',\n",
    "    '''CREATE TABLE IF NOT EXISTS appointments (\n",
    "           id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "           patient_id INTEGER NOT NULL,\n",
    "           doctor_id INTEGER NOT NULL,\n",
    "           date DATE NOT NULL,\n",
    "           time_slot TEXT NOT NULL,\n",
    "           FOREIGN KEY (patient_id) REFERENCES users(id),\n",
    "           FOREIGN KEY (doctor_id) REFERENCES doctors(id)\n",
    "       );''',\n",
    "    '''CREATE TABLE IF NOT EXISTS time_slots (\n",
    "           time_slot TEXT PRIMARY KEY\n",
    "       );'''\n",
    "]\n",
    "\n",
    "# SQL command to insert initial data\n",
    "insert_initial_data_commands = [\n",
    "    \"INSERT INTO doctors (name, specialization) VALUES ('Dr. Smith', 'Cardiology');\",\n",
    "    \"INSERT INTO doctors (name, specialization) VALUES ('Dr. Jones', 'Neurology');\",\n",
    "    \"INSERT INTO time_slots (time_slot) VALUES ('09:00');\",\n",
    "    \"INSERT INTO time_slots (time_slot) VALUES ('10:00');\"\n",
    "    # ... more inserts as needed ...\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    # Execute table creation commands\n",
    "    for command in create_tables_commands:\n",
    "        cursor.execute(command)\n",
    "\n",
    "    # Execute data insertion commands\n",
    "    for command in insert_initial_data_commands:\n",
    "        cursor.execute(command)\n",
    "\n",
    "    # Commit changes and close connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "        # Execute your commands here\n",
    "except sqlite3.Error as e:\n",
    "    print(\"SQLite error:\", e)\n",
    "finally:\n",
    "    if conn:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table: users\n",
      "(1, 'alice', 'hashed_password', 'patient', 'alice@example.com', '1234567890', 30, 'A+')\n",
      "(2, 'bob', 'hashed_password', 'doctor', 'bob@example.com', '0987654321', 45, 'O-')\n",
      "(3, 'prakash', '03ac674216f3e15c761ee1a5e255f067953623c8b388b4459e13f978d7c846f4', 'doctor', None, None, None, None)\n",
      "(4, 'ibna', '03ac674216f3e15c761ee1a5e255f067953623c8b388b4459e13f978d7c846f4', 'patient', None, None, None, None)\n",
      "\n",
      "Table: doctors\n",
      "(1, None, 'Dr. Bob', 'Cardiology')\n",
      "(2, None, 'Dr. Susan', 'Dermatology')\n",
      "(3, 3, 'Prakash D', 'Cardiology')\n",
      "\n",
      "Table: appointments\n",
      "(1, '', 4, '09:00', '2023-11-28')\n",
      "(2, '', 4, '10:00', '2023-11-28')\n",
      "(3, '', 3, '09:00', '2023-11-28')\n",
      "(4, '', 4, '09:00', '2023-11-28')\n",
      "\n",
      "Table: time_slots\n",
      "(1, '09:00')\n",
      "(2, '10:00')\n",
      "(3, '11:00')\n",
      "(4, '14:00')\n",
      "(5, '15:00')\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('medical_app_new.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch data from all tables\n",
    "tables = ['users', 'doctors', 'appointments', 'time_slots']\n",
    "data = {}\n",
    "\n",
    "for table in tables:\n",
    "    cursor.execute(f\"SELECT * FROM {table}\")\n",
    "    data[table] = cursor.fetchall()\n",
    "\n",
    "# Fetch a specific doctor by name\n",
    "# cursor.execute(\"SELECT id FROM doctors WHERE name = ?\", ('Prakash',))\n",
    "# doctor_id = cursor.fetchone()\n",
    "# if doctor_id:\n",
    "#     doctor_id = doctor_id[0]  # Extract the ID\n",
    "#     print(f\"Doctor ID: {doctor_id}\")\n",
    "# else:\n",
    "#     print(\"Doctor not found\")\n",
    "# cursor.execute(\"SELECT * FROM USERS\")\n",
    "# samples = cursor.fetchone()\n",
    "# print(samples)\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Displaying the fetched data\n",
    "for table, rows in data.items():\n",
    "    print(f\"\\nTable: {table}\")\n",
    "    for row in rows:\n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created and populated with dummy data.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create or open a database\n",
    "conn = sqlite3.connect('medical_app_new.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL script to create tables\n",
    "create_tables_script = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS users (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    username TEXT UNIQUE,\n",
    "    password_hash TEXT,\n",
    "    role TEXT,\n",
    "    email TEXT,\n",
    "    phone TEXT,\n",
    "    age INTEGER,\n",
    "    blood_group TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS doctors (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    user_id INTEGER,\n",
    "    name TEXT,\n",
    "    specialization TEXT,\n",
    "    FOREIGN KEY (user_id) REFERENCES users(id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS appointments (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    doctor_id INTEGER,\n",
    "    patient_id INTEGER,\n",
    "    time_slot TEXT,\n",
    "    date TEXT,\n",
    "    FOREIGN KEY (doctor_id) REFERENCES doctors(id),\n",
    "    FOREIGN KEY (patient_id) REFERENCES users(id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS time_slots (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    time_slot TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL script to create tables\n",
    "cursor.executescript(create_tables_script)\n",
    "\n",
    "# Insert dummy data\n",
    "insert_data_script = \"\"\"\n",
    "INSERT INTO users (username, password_hash, role, email, phone, age, blood_group) VALUES\n",
    "('alice', 'hashed_password', 'patient', 'alice@example.com', '1234567890', 30, 'A+'),\n",
    "('bob', 'hashed_password', 'doctor', 'bob@example.com', '0987654321', 45, 'O-');\n",
    "\n",
    "INSERT INTO doctors (name, specialization) VALUES\n",
    "('Dr. Bob', 'Cardiology'),\n",
    "('Dr. Susan', 'Dermatology');\n",
    "\n",
    "INSERT INTO time_slots (time_slot) VALUES\n",
    "('09:00'),\n",
    "('10:00'),\n",
    "('11:00'),\n",
    "('14:00'),\n",
    "('15:00');\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL script to insert data\n",
    "cursor.executescript(insert_data_script)\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"Database created and populated with dummy data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "no such table: users",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     conn\u001b[39m.\u001b[39mcommit()\n\u001b[1;32m      8\u001b[0m     conn\u001b[39m.\u001b[39mclose()\n\u001b[0;32m---> 10\u001b[0m add_role_column()\n",
      "Cell \u001b[0;32mIn[27], line 6\u001b[0m, in \u001b[0;36madd_role_column\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m conn \u001b[39m=\u001b[39m sqlite3\u001b[39m.\u001b[39mconnect(\u001b[39m'\u001b[39m\u001b[39musers.db\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m c \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mcursor()\n\u001b[0;32m----> 6\u001b[0m c\u001b[39m.\u001b[39;49mexecute(\u001b[39m\"\u001b[39;49m\u001b[39mALTER TABLE users ADD COLUMN role TEXT;\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m conn\u001b[39m.\u001b[39mcommit()\n\u001b[1;32m      8\u001b[0m conn\u001b[39m.\u001b[39mclose()\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: users"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "def add_role_column():\n",
    "    conn = sqlite3.connect('users.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"ALTER TABLE users ADD COLUMN role TEXT;\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "add_role_column()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created and populated with dummy data.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create or open a database called 'medical.db'\n",
    "conn = sqlite3.connect('medical.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL script to create tables\n",
    "sql_script = \"\"\"\n",
    "-- Users Table\n",
    "CREATE TABLE IF NOT EXISTS users (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    username TEXT UNIQUE NOT NULL,\n",
    "    password_hash TEXT NOT NULL,\n",
    "    role TEXT NOT NULL,\n",
    "    age INTEGER,\n",
    "    email TEXT,\n",
    "    blood_group TEXT,\n",
    "    phone TEXT\n",
    ");\n",
    "\n",
    "-- Doctors Table\n",
    "CREATE TABLE IF NOT EXISTS doctors (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    name TEXT NOT NULL,\n",
    "    specialization TEXT NOT NULL,\n",
    "    email TEXT,\n",
    "    phone TEXT\n",
    ");\n",
    "\n",
    "-- Timeslots Table\n",
    "CREATE TABLE IF NOT EXISTS timeslots (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    doctor_id INTEGER,\n",
    "    time_slot TEXT NOT NULL,\n",
    "    FOREIGN KEY (doctor_id) REFERENCES doctors(id)\n",
    ");\n",
    "\n",
    "-- Appointments Table\n",
    "CREATE TABLE IF NOT EXISTS appointments (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    user_id INTEGER,\n",
    "    doctor_id INTEGER,\n",
    "    timeslot_id INTEGER,\n",
    "    status TEXT,\n",
    "    FOREIGN KEY (user_id) REFERENCES users(id),\n",
    "    FOREIGN KEY (doctor_id) REFERENCES doctors(id),\n",
    "    FOREIGN KEY (timeslot_id) REFERENCES timeslots(id)\n",
    ");\n",
    "\n",
    "-- Insert some dummy data\n",
    "-- Users\n",
    "INSERT INTO users (username, password_hash, role, age, email, blood_group, phone) VALUES \n",
    "('Alice', 'hashed_password', 'patient', 30, 'alice@example.com', 'A+', '1234567890'),\n",
    "('Bob', 'hashed_password', 'patient', 25, 'bob@example.com', 'B+', '0987654321');\n",
    "\n",
    "-- Doctors\n",
    "INSERT INTO doctors (name, specialization, email, phone) VALUES \n",
    "('Dr. Smith', 'Cardiology', 'smith@example.com', '1112223333'),\n",
    "('Dr. Johnson', 'Neurology', 'johnson@example.com', '2223334444');\n",
    "\n",
    "-- Timeslots\n",
    "INSERT INTO timeslots (doctor_id, time_slot) VALUES \n",
    "(1, '2023-04-01 10:00:00'),\n",
    "(1, '2023-04-01 11:00:00'),\n",
    "(2, '2023-04-01 10:00:00'),\n",
    "(2, '2023-04-01 11:00:00');\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL script\n",
    "cursor.executescript(sql_script)\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "print(\"Database created and populated with dummy data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.7320,  0.2005, -0.4418,  ..., -0.0720,  0.4337, -0.7020],\n",
      "         [ 0.3279, -0.6177, -0.2911,  ...,  0.2360, -0.3052, -0.5538],\n",
      "         [ 0.3102,  0.4961,  0.5651,  ...,  0.3165,  0.1047, -0.3785],\n",
      "         ...,\n",
      "         [ 0.5009, -0.5723, -0.1991,  ...,  0.7713, -0.0373, -0.6282],\n",
      "         [ 0.2451, -0.2987, -0.3943,  ...,  0.3207, -0.1525, -0.2628],\n",
      "         [ 1.0569,  0.3928, -1.0537,  ...,  0.3469, -0.0479, -0.7335]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load ClinicalBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "# Example medical text and question\n",
    "medical_text = \"Patient shows symptoms of flu. Prescribed medication includes ibuprofen and rest.\"\n",
    "question = \"What symptoms does the patient show?\"\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    return inputs\n",
    "\n",
    "medical_text_inputs = preprocess(medical_text)\n",
    "question_inputs = preprocess(question)\n",
    "\n",
    "# Extract embeddings\n",
    "with torch.no_grad():\n",
    "    medical_text_embeddings = model(**medical_text_inputs).last_hidden_state\n",
    "    question_embeddings = model(**question_inputs).last_hidden_state\n",
    "    # print(question_embeddings)\n",
    "\n",
    "# Further steps would involve comparing these embeddings to find the most relevant parts of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. check_pairwise_arrays expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m sentence_embs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([get_embeddings(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m medical_text])\n\u001b[1;32m     24\u001b[0m \u001b[39m# Compute cosine similarity and find the most relevant sentence\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m similarities \u001b[39m=\u001b[39m cosine_similarity(question_emb, sentence_embs)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m most_relevant_idx \u001b[39m=\u001b[39m similarities\u001b[39m.\u001b[39margmax()\n\u001b[1;32m     27\u001b[0m answer \u001b[39m=\u001b[39m medical_text[most_relevant_idx]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    215\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/sklearn/metrics/pairwise.py:1578\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1543\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1544\u001b[0m \n\u001b[1;32m   1545\u001b[0m \u001b[39mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1574\u001b[0m \u001b[39m    Returns the cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1575\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1576\u001b[0m \u001b[39m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m-> 1578\u001b[0m X, Y \u001b[39m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[1;32m   1580\u001b[0m X_normalized \u001b[39m=\u001b[39m normalize(X, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1581\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m Y:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/sklearn/metrics/pairwise.py:173\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    166\u001b[0m         X,\n\u001b[1;32m    167\u001b[0m         accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[1;32m    172\u001b[0m     )\n\u001b[0;32m--> 173\u001b[0m     Y \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    174\u001b[0m         Y,\n\u001b[1;32m    175\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m    176\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    177\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    178\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m    179\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    180\u001b[0m     )\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m precomputed:\n\u001b[1;32m    183\u001b[0m     \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/sklearn/utils/validation.py:951\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    947\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    948\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    949\u001b[0m     )\n\u001b[1;32m    950\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nd \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m--> 951\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    952\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m         \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    954\u001b[0m     )\n\u001b[1;32m    956\u001b[0m \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m    957\u001b[0m     _assert_all_finite(\n\u001b[1;32m    958\u001b[0m         array,\n\u001b[1;32m    959\u001b[0m         input_name\u001b[39m=\u001b[39minput_name,\n\u001b[1;32m    960\u001b[0m         estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[1;32m    961\u001b[0m         allow_nan\u001b[39m=\u001b[39mforce_all_finite \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    962\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. check_pairwise_arrays expected <= 2."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load ClinicalBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "# Function to preprocess text and extract embeddings\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)  # Mean pooling\n",
    "\n",
    "# Sample medical text split into sentences and a sample question\n",
    "medical_text = [\"Patient shows symptoms of flu.\", \"Prescribed medication includes ibuprofen and rest.\"]\n",
    "question = \"What medication was prescribed?\"\n",
    "\n",
    "# Get embeddings\n",
    "question_emb = get_embeddings(question)\n",
    "sentence_embs = torch.stack([get_embeddings(sentence) for sentence in medical_text])\n",
    "\n",
    "# Compute cosine similarity and find the most relevant sentence\n",
    "similarities = cosine_similarity(question_emb, sentence_embs)[0]\n",
    "most_relevant_idx = similarities.argmax()\n",
    "answer = medical_text[most_relevant_idx]\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# summarizer = pipeline(\"summarization\", model=\"your/medical_text_summarization_model\")\n",
    "\n",
    "MEDICAL_DOCUMENT = \"\"\" \n",
    "duplications of the alimentary tract are well - known but rare congenital malformations that can occur anywhere in the gastrointestinal ( gi ) tract from the tongue to the anus . while midgut duplications are the most common , foregut duplications such as oesophagus , stomach , and parts 1 and 2 of the duodenum account for approximately one - third of cases . \n",
    " they are most commonly seen either in the thorax or abdomen or in both as congenital thoracoabdominal duplications . \n",
    " cystic oesophageal duplication ( ced ) , the most common presentation , is often found in the lower third part ( 60 - 95% ) and on the right side [ 2 , 3 ] . hydatid cyst ( hc ) is still an important health problem throughout the world , particularly in latin america , africa , and mediterranean areas . \n",
    " turkey , located in the mediterranean area , shares this problem , with an estimated incidence of 20/100 000 . \n",
    " most commonly reported effected organ is liver , but in children the lungs are the second most frequent site of involvement [ 4 , 5 ] . in both ced and hc , the presentation depends on the site and the size of the cyst . \n",
    " hydatid cysts are far more common than other cystic intrathoracic lesions , especially in endemic areas , so it is a challenge to differentiate ced from hc in these countries . here , \n",
    " we present a 7-year - old girl with intrathoracic cystic mass lesion , who had been treated for hydatid cyst for 9 months , but who turned out to have oesophageal cystic duplication . \n",
    " a 7-year - old girl was referred to our clinic with coincidentally established cystic intrathoracic lesion during the investigation of aetiology of anaemia . \n",
    " the child was first admitted with loss of vision in another hospital ten months previously . \n",
    " the patient 's complaints had been attributed to pseudotumour cerebri due to severe iron deficiency anaemia ( haemoglobin : 3 g / dl ) . \n",
    " chest radiography and computed tomography ( ct ) images resulted in a diagnosis of cystic intrathoracic lesion ( fig . \n",
    " the cystic mass was accepted as a type 1 hydatid cyst according to world health organization ( who ) classification . \n",
    " after 9 months of medication , no regression was detected in ct images , so the patient was referred to our department . \n",
    " an ondirect haemagglutination test result was again negative . during surgery , after left thoracotomy incision , a semi - mobile cystic lesion , which was almost seven centimetres in diameter , with smooth contour , was found above the diaphragm , below the lung , outside the pleura ( fig . \n",
    " the entire fluid in the cyst was aspirated ; it was brown and bloody ( fig . \n",
    " 2 ) . the diagnosis of cystic oesophageal duplication was considered , and so an attachment point was searched for . \n",
    " it was below the hiatus , on the lower third left side of the oesophagus , and it also was excised completely through the hiatus . \n",
    " pathologic analysis of the specimen showed oesophageal mucosa with an underlying proper smooth muscle layer . \n",
    " computed tomography image of the cystic intrathoracic lesion cystic lesion with brownish fluid in the cyst \n",
    " compressible organs facilitate the growth of the cyst , and this has been proposed as a reason for the apparent prevalence of lung involvement in children . diagnosis is often incidental and can be made with serological tests and imaging [ 5 , 7 ] . \n",
    " laboratory investigations include the casoni and weinberg skin tests , indirect haemagglutination test , elisa , and the presence of eosinophilia , but can be falsely negative because children may have a poor serological response to eg . \n",
    " false - positive reactions are related to the antigenic commonality among cestodes and conversely seronegativity can not exclude hydatidosis . \n",
    " false - negative results are observed when cysts are calcified , even if fertile [ 4 , 8 ] . in our patient iha levels were negative twice . \n",
    " due to the relatively non - specific clinical signs , diagnosis can only be made confidently using appropriate imaging . \n",
    " plain radiographs , ultrasonography ( us ) , or ct scans are sufficient for diagnosis , but magnetic resonance imaging ( mri ) is also very useful [ 5 , 9 ] . \n",
    " computed tomography demonstrates cyst wall calcification , infection , peritoneal seeding , bone involvement fluid density of intact cysts , and the characteristic internal structure of both uncomplicated and ruptured cysts [ 5 , 9 ] . \n",
    " the conventional treatment of hydatid cysts in all organs is surgical . in children , small hydatid cysts of the lungs \n",
    " respond favourably to medical treatment with oral administration of certain antihelminthic drugs such as albendazole in certain selected patients . \n",
    " the response to therapy differs according to age , cyst size , cyst structure ( presence of daughter cysts inside the mother cysts and thickness of the pericystic capsule allowing penetration of the drugs ) , and localization of the cyst . in children , small cysts with thin pericystic capsule localised in the brain and lungs respond favourably [ 6 , 11 ] . \n",
    " respiratory symptoms are seen predominantly in cases before two years of age . in our patient , who has vision loss , the asymptomatic duplication cyst was found incidentally . \n",
    " the lesion occupied the left hemithorax although the most common localisation reported in the literature is the lower and right oesophagus . \n",
    " the presentation depends on the site and the size of the malformations , varying from dysphagia and respiratory distress to a lump and perforation or bleeding into the intestine , but cysts are mostly diagnosed incidentally . \n",
    " if a cystic mass is suspected in the chest , the best technique for evaluation is ct . \n",
    " magnetic resonance imaging can be used to detail the intimate nature of the cyst with the spinal canal . \n",
    " duplications should have all three typical signs : first of all , they should be attached to at least one point of the alimentary tract ; second and third are that they should have a well - developed smooth muscle coat , and the epithelial lining of duplication should represent some portions of alimentary tract , respectively [ 2 , 10 , 12 ] . in summary , the cystic appearance of both can cause a misdiagnosis very easily due to the rarity of cystic oesophageal duplications as well as the higher incidence of hydatid cyst , especially in endemic areas . \n",
    "\"\"\"\n",
    "# print(summarizer(MEDICAL_DOCUMENT, max_length=230, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "# print(summarizer(MEDICAL_DOCUMENT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 20, but your input_length is only 4. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 20, but your input_length is only 4. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=2)\n",
      "Your max_length is set to 20, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 20, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 20, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 20, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 20, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 20, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 20, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(i)\n\u001b[0;32m----> 6\u001b[0m candidate \u001b[39m=\u001b[39m summarizer(text, min_length\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, max_length\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m candidate_summaries\u001b[39m.\u001b[39mappend(candidate[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msummary_text\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/transformers/pipelines/text2text_generation.py:265\u001b[0m, in \u001b[0;36mSummarizationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    242\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39m    Summarize the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m          ids of the summary.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/transformers/pipelines/text2text_generation.py:165\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    137\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    167\u001b[0m         \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m)\n\u001b[1;32m    168\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(el, \u001b[39mstr\u001b[39m) \u001b[39mfor\u001b[39;00m el \u001b[39min\u001b[39;00m args[\u001b[39m0\u001b[39m])\n\u001b[1;32m    169\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(res) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result)\n\u001b[1;32m    170\u001b[0m     ):\n\u001b[1;32m    171\u001b[0m         \u001b[39mreturn\u001b[39;00m [res[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/transformers/pipelines/base.py:1129\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[39melif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1127\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1128\u001b[0m     )\n\u001b[0;32m-> 1129\u001b[0m \u001b[39melif\u001b[39;00m is_iterable:\n\u001b[1;32m   1130\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1131\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ChunkPipeline):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/transformers/pipelines/base.py:1136\u001b[0m, in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1131\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ChunkPipeline):\n\u001b[1;32m   1132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1135\u001b[0m                 [inputs], num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[0;32m-> 1136\u001b[0m             )\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/transformers/pipelines/base.py:1035\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_inference_context\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m-> 1035\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/transformers/pipelines/text2text_generation.py:187\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m generate_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmax_length)\n\u001b[1;32m    186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_inputs(input_length, generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m], generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 187\u001b[0m output_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m    188\u001b[0m out_b \u001b[39m=\u001b[39m output_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/transformers/generation/utils.py:1675\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1664\u001b[0m beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1665\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1666\u001b[0m     num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1671\u001b[0m     max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1672\u001b[0m )\n\u001b[1;32m   1673\u001b[0m \u001b[39m# 12. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   1674\u001b[0m input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m-> 1675\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1676\u001b[0m     expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1677\u001b[0m     is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1678\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1679\u001b[0m )\n\u001b[1;32m   1680\u001b[0m \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_search(\n\u001b[1;32m   1682\u001b[0m     input_ids,\n\u001b[1;32m   1683\u001b[0m     beam_scorer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1692\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/transformers/generation/utils.py:3014\u001b[0m, in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3012\u001b[0m # send 0.0 if we finished, 1.0 otherwise\n\u001b[1;32m   3013\u001b[0m dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n\u001b[0;32m-> 3014\u001b[0m # did all peers finish? the reduced sum will be 0.0 then\n\u001b[1;32m   3015\u001b[0m if this_peer_finished_flag.item() == 0.0:\n\u001b[1;32m   3016\u001b[0m     break\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1497\u001b[0m     return self.forward(*input, **kwargs)\n\u001b[1;32m   1498\u001b[0m recording_scopes = torch.jit._trace._trace_module_map is not None\n\u001b[1;32m   1499\u001b[0m if recording_scopes:\n\u001b[1;32m   1500\u001b[0m     # type ignore was added because at this point one knows that\n\u001b[0;32m-> 1501\u001b[0m     # torch.jit._trace._trace_module_map is not Optional and has type Dict[Any, Any]\n\u001b[1;32m   1502\u001b[0m     name = torch.jit._trace._trace_module_map[self] if self in torch.jit._trace._trace_module_map else None  # type: ignore[index, operator] # noqa: B950\n\u001b[1;32m   1503\u001b[0m     if name:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py:1388\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1383\u001b[0m     \u001b[39mif\u001b[39;00m decoder_input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m decoder_inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1384\u001b[0m         decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1385\u001b[0m             labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1386\u001b[0m         )\n\u001b[0;32m-> 1388\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1389\u001b[0m     input_ids,\n\u001b[1;32m   1390\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1391\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1392\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[1;32m   1393\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1394\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1395\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1396\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1397\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1398\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1399\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1400\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1401\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1402\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1403\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1404\u001b[0m )\n\u001b[1;32m   1406\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m])\n\u001b[1;32m   1407\u001b[0m lm_logits \u001b[39m=\u001b[39m lm_logits \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_logits_bias\u001b[39m.\u001b[39mto(lm_logits\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1497\u001b[0m     return self.forward(*input, **kwargs)\n\u001b[1;32m   1498\u001b[0m recording_scopes = torch.jit._trace._trace_module_map is not None\n\u001b[1;32m   1499\u001b[0m if recording_scopes:\n\u001b[1;32m   1500\u001b[0m     # type ignore was added because at this point one knows that\n\u001b[0;32m-> 1501\u001b[0m     # torch.jit._trace._trace_module_map is not Optional and has type Dict[Any, Any]\n\u001b[1;32m   1502\u001b[0m     name = torch.jit._trace._trace_module_map[self] if self in torch.jit._trace._trace_module_map else None  # type: ignore[index, operator] # noqa: B950\n\u001b[1;32m   1503\u001b[0m     if name:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py:1274\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1268\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[1;32m   1269\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1270\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1271\u001b[0m     )\n\u001b[1;32m   1273\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1274\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   1275\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1276\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1277\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_outputs[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m   1278\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1279\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1280\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1281\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1282\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1283\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1284\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1285\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1286\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1287\u001b[0m )\n\u001b[1;32m   1289\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1290\u001b[0m     \u001b[39mreturn\u001b[39;00m decoder_outputs \u001b[39m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1497\u001b[0m     return self.forward(*input, **kwargs)\n\u001b[1;32m   1498\u001b[0m recording_scopes = torch.jit._trace._trace_module_map is not None\n\u001b[1;32m   1499\u001b[0m if recording_scopes:\n\u001b[1;32m   1500\u001b[0m     # type ignore was added because at this point one knows that\n\u001b[0;32m-> 1501\u001b[0m     # torch.jit._trace._trace_module_map is not Optional and has type Dict[Any, Any]\n\u001b[1;32m   1502\u001b[0m     name = torch.jit._trace._trace_module_map[self] if self in torch.jit._trace._trace_module_map else None  # type: ignore[index, operator] # noqa: B950\n\u001b[1;32m   1503\u001b[0m     if name:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py:1132\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1121\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m   1122\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m   1123\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1130\u001b[0m     )\n\u001b[1;32m   1131\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m   1133\u001b[0m         hidden_states,\n\u001b[1;32m   1134\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1135\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1136\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1137\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1138\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49m(\n\u001b[1;32m   1139\u001b[0m             cross_attn_head_mask[idx] \u001b[39mif\u001b[39;49;00m cross_attn_head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m   1140\u001b[0m         ),\n\u001b[1;32m   1141\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   1142\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1143\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1144\u001b[0m     )\n\u001b[1;32m   1145\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1497\u001b[0m     return self.forward(*input, **kwargs)\n\u001b[1;32m   1498\u001b[0m recording_scopes = torch.jit._trace._trace_module_map is not None\n\u001b[1;32m   1499\u001b[0m if recording_scopes:\n\u001b[1;32m   1500\u001b[0m     # type ignore was added because at this point one knows that\n\u001b[0;32m-> 1501\u001b[0m     # torch.jit._trace._trace_module_map is not Optional and has type Dict[Any, Any]\n\u001b[1;32m   1502\u001b[0m     name = torch.jit._trace._trace_module_map[self] if self in torch.jit._trace._trace_module_map else None  # type: ignore[index, operator] # noqa: B950\n\u001b[1;32m   1503\u001b[0m     if name:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py:463\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n\u001b[1;32m    462\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m--> 463\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_fn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(hidden_states))\n\u001b[1;32m    464\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_dropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m    465\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(hidden_states)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1497\u001b[0m     return self.forward(*input, **kwargs)\n\u001b[1;32m   1498\u001b[0m recording_scopes = torch.jit._trace._trace_module_map is not None\n\u001b[1;32m   1499\u001b[0m if recording_scopes:\n\u001b[1;32m   1500\u001b[0m     # type ignore was added because at this point one knows that\n\u001b[0;32m-> 1501\u001b[0m     # torch.jit._trace._trace_module_map is not Optional and has type Dict[Any, Any]\n\u001b[1;32m   1502\u001b[0m     name = torch.jit._trace._trace_module_map[self] if self in torch.jit._trace._trace_module_map else None  # type: ignore[index, operator] # noqa: B950\n\u001b[1;32m   1503\u001b[0m     if name:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/med_env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "candidate_summaries = []\n",
    "\n",
    "for i, text in enumerate(MEDICAL_DOCUMENT):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    candidate = summarizer(text, min_length=5, max_length=20)\n",
    "    candidate_summaries.append(candidate[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    \"I\\'m sorry I didn\\'t get a picture of the',\n",
       " 'CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery.',\n",
       " 'D. d. d',\n",
       " 'U.S. President Barack Obama has been criticized for his handling of the economy.',\n",
       " ' p p p. p.',\n",
       " ' l l l. l.',\n",
       " 'i. i i i. i.',\n",
       " 'Cecile Richards is the author of the book \"Cecil Richards: A']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
